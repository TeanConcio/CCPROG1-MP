{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Introduction to the Problem and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The garment industry is a labor-intensive sector heavily dependent on manual processes. This reliance presents several challenges, including maintaining high levels of productivity, ensuring consistent quality, and managing the workforce efficiently. With increasing competition and the need for rapid turnaround times, optimizing these processes becomes crucial for industry stakeholders. As such, tracking, analyzing, and predicting productivity and performance of factory teams is highly desirable. With that in mind, the problem/task that will be targeted in this notebook is a regression problem focusing on predicting productivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Description of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Garments dataset is a dataset that contains important attributes of the garment manufacturing process and employee productivity. This dataset was manually collected by Abdulla Al Imran and then validated by industry experts. As the data was manually collected, there is a possibility that the accuracy and reliability of the data is affected. However, as the data was validated by industry experts, the impact on accuracy and reliability of the data may be lessened. \n",
    "\n",
    "In the dataset, each row represents an instance identified by a combination of three features: date, team, and department. Meanwhile, each column represents a feature. There are a total of 1197 instances in this dataset and 15 features. The features are as follows:\n",
    "\n",
    "- date: the working day from when the data is from\n",
    "- quarter: the portion of the month. In the context of this dataset, it serves as 7 day intervals of a month meaning months reaching over 28 days may have a 5th quarter.\n",
    "- department: the department associated with the rest of the data of the instance\n",
    "- day: the day of the week\n",
    "- team: the team number ass\n",
    "- targeted_productivity: the targeted productivity set by those in charge of each team for the day.\n",
    "- smv: standard minute value, the amount of time required to complete a task under standard working conditions\n",
    "- wip: work in progress, the amount of items that are still being worked on as well as unfinished items\n",
    "- over_time: the amount of overtime spent by each team in total in minutes\n",
    "- incentive: the amount of financial incentive (specifically BDT) used to motivate workers\n",
    "- idle_time: the amount of times when production was interrupted during the day\n",
    "- idle_men: the number of workers who were idle because of production interruption\n",
    "- actual_productivity: the % of productivity delivered by the workers (0-1 scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. List of requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the path for the dataset as well as the random_seed to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "dataset_path = 'Dataset 2 - Garments Dataset/garments.csv'\n",
    "random_seed = 69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries used include:\n",
    "\n",
    "- pandas\n",
    "- numpy\n",
    "- matplotlib\n",
    "- itertools\n",
    "- seaborn\n",
    "- sklearn\n",
    "- joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "\n",
    "# Training and Testing the model\n",
    "from sklearn.model_selection import train_test_split # For splitting the dataset into training and testing\n",
    "from sklearn.pipeline import Pipeline # For creating a pipeline\n",
    "from sklearn.compose import ColumnTransformer # For transforming the columns\n",
    "from sklearn.preprocessing import StandardScaler # For Standardization\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV # For Hyperparameter Tuning\n",
    "from sklearn.preprocessing import PolynomialFeatures  # For Polynomial Regression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score   # For evaluation\n",
    "import joblib   # For saving the model\n",
    "from sklearn.base import BaseEstimator, RegressorMixin # For creating a custom regressor\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LinearRegression # For Linear Regression\n",
    "from sklearn.linear_model import Lasso # For Lasso Regression\n",
    "from sklearn.linear_model import SGDRegressor # For Stochastic Gradient Descent\n",
    "from sklearn.neural_network import MLPRegressor # For Neural Network\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load the dataset from Dataset folder\n",
    "og_df = pd.read_csv(dataset_path)\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "pd.set_option('display.max_columns', None) # Display all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Data Preprocessing and Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaN values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check values in wip\n",
    "df['wip'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the null values with the mean of the column. Setting the mean as the fill value instead of 0 is done as the feature is 'WIP', a feature representing the unfinished items at some stage of production in the garment factory. Since it's improbable that the WIP value is zero (as long as production is ongoing), filling null values with the column's mean is a reasonable approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the missing values with the mean of the column\n",
    "df['wip'].fillna(df['wip'].mean(), inplace=True)\n",
    "df['wip'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The department column has typos and inconsistencies so it will be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check department column\n",
    "print(df['department'].unique())\n",
    "\n",
    "# change all 'finishing ' to 'finishing'\n",
    "df['department'] = df['department'].replace('finishing ', 'finishing')\n",
    "df['department'] = df['department'].replace('sweing', 'sewing')\n",
    "print(df['department'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the data type of all the columns to ensure that they are in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check datatypes for all columns\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the values of the features to ensure that there are no anomalies in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['department'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['quarter'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['day'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['team'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['targeted_productivity'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['smv'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['over_time'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are instances of no_of_workers having half values. This is not possible in a real-world scenario so the values will be rounded down to the nearest whole number. We round down as a conservative estimation to avoid overestimating the number of workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['no_of_workers'].head())\n",
    "# round down the no_of_workers\n",
    "df['no_of_workers'] = np.floor(df['no_of_workers'])\n",
    "print(df['no_of_workers'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there are any instances where actual productivity exceeds the 0-1 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['actual_productivity'].value_counts())\n",
    "# print actual_productivity where it is < 0 or > 1\n",
    "print(df[(df['actual_productivity'] < 0) | (df['actual_productivity'] > 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since actual productivity has values exceeding 1, we round them to 1. We do this since the dataset description states that the range of actual productivity is 0-1. This is done to ensure that the model is trained on the correct range of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round down the actual_productivity if it is > 1\n",
    "df['actual_productivity'] = np.where(df['actual_productivity'] > 1, 1, df['actual_productivity'])\n",
    "print(df[(df['actual_productivity'] < 0) | (df['actual_productivity'] > 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the date column as the data only spans a single season, making it unlikely to provide meaningful variability or insights for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop date\n",
    "df = df.drop('date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if any numerical value is less than 0\n",
    "print(df[(df.select_dtypes(include=[np.number]) < 0).any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all datatypes\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check for large over_time values but do not remove them yet as the data may be valid and the numbers are believable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate overtime per worker\n",
    "df['overtime_per_worker'] = df['over_time'] / df['no_of_workers']\n",
    "print(df[df['overtime_per_worker'] > 120].shape)\n",
    "print(df[df['overtime_per_worker'] > 240].shape)\n",
    "print(df[df['overtime_per_worker'] > 420].shape)\n",
    "print(df[df['overtime_per_worker'] > 600].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all idle_men with decimal values\n",
    "print(df[df['idle_men'] % 1 != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print all rows with idle time = 0 and idle_men > 0\n",
    "print(df[(df['idle_time'] == 0) & (df['idle_men'] > 0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since department, day, and quarter are categorical variables, we one-hot encode them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn department, day, and quarter into one-hot encoding\n",
    "df_cleaned = df.copy()\n",
    "df = pd.get_dummies(df, columns=['department', 'day', 'quarter'])\n",
    "# print all datatypes\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check for days where there are no incentives but there is overtime. However, we do not remove these instances as incentives may not be the only reason for overtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for days where there are no incentives but have overtime\n",
    "df[(df['incentive'] == 0) & (df['over_time'] > 0)][['targeted_productivity', 'idle_men', 'idle_time', 'actual_productivity', 'incentive', 'over_time', 'wip', 'smv']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check cleaned dataset\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V-A. Correlation and Distribution of Numerical Features (Seal)\n",
    "\n",
    "A histogram and boxplot is used to determine the distribution of the data and outlier detection for all numerical features in the data. As observed below, numerical features in the dataset have significantly different ranges.  Large-scale differences can bias these algorithms toward features with larger magnitudes.\n",
    "\n",
    "Hence, there is a major Consideration to use Standard Scaler as it centers the data at 0 and scales all features to a standard deviation of 1, ensuring all features contribute equally to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_numerics(data):\n",
    "    numeric_columns = data.columns\n",
    "    # Plotting histograms and box plots for each numeric column\n",
    "    for column in numeric_columns:\n",
    "        _, ax = plt.subplots(1,2, figsize=(16, 5))\n",
    "        ax=ax.flatten()\n",
    "        # Histogram\n",
    "        sns.histplot(data[column], bins=50, kde=True, color='skyblue', ax=ax[0])\n",
    "        ax[0].set_title(f'Histogram of {column}', fontsize=15,fontweight='bold')\n",
    "        ax[0].set_xlabel(column, fontsize=12)\n",
    "        ax[0].set_ylabel('Frequency', fontsize=12)\n",
    "        # Box plot\n",
    "        sns.boxplot(x=data[column], color='#FFEE8C', ax=ax[1])\n",
    "        ax[1].set_title(f'Box plot of {column}', fontsize=15,fontweight='bold')\n",
    "        ax[1].set_xlabel(column, fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will plot the numerical features to see the distribution of the data and to check for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['targeted_productivity', 'smv', 'wip', 'over_time', 'incentive', 'idle_time', 'actual_productivity', 'overtime_per_worker']\n",
    "plot_numerics(df[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of targeted productivity implies that teams are expected to perform at a high level, with majority of the data points falling between 0.7 and 0.8. As these are target values in a production setting, it is expected that they are set at a high level to ensure production efficiency.\n",
    "\n",
    "On the other hand, actual productivity is more spread out, with a wider range of values possibly indicating that teams are unable to meet the high targets set for them. This could be due to various factors such as lack of incentives, high idle time, or high overtime.\n",
    "\n",
    "Incentive and idle_time have mostly 0 values. This indicates that incentives are rarely given and production interruptions are infrequent.\n",
    "\n",
    "Finally, overtime_per_worker shows that teams tend to work overtime, with most values falling between 100 to 200 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will look at the correlation between the numerical features to see if there are any strong correlations between the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df[num_cols].corr()\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based from the correlation matrix, we can see that most of the features have a weak correlation with each other. The highest correlation is between 'over_time' and 'smv' implying that the workers are more likely to work overtime if the tasks for the day are more time-consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V-B. Productivity with regards to Teams and Departments (Orrin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we want to look at how productivity varies with respect to teams and departments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual productivity vs department\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.boxplot(x='department', y='actual_productivity', data=df_cleaned)\n",
    "plt.title(\"Actual Productivity vs Department\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the finishing department has a higher median productivity than the sewing department but it also has higher variability. Furthermore, the sewing department has multiple outliers on the lower end of the productivity scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain more insights, we look into the productivity of each team in the sewing and finishing departments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual productivity vs team by department\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.boxplot(x='team', y='actual_productivity', hue='department', data=df_cleaned)\n",
    "plt.title(\"Actual Productivity vs Team by Department\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finishing department consistently shows higher median productivity with less variability, while the sewing department has lower productivity and more extreme outliers, indicating greater inconsistency. \n",
    "\n",
    "Team 6 of the finishing department and team 5 of the sewing department stand out as underperformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is a target productivity set for each team, we can compare the actual productivity with the target productivity to see how well each team is performing relative to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# productivity diff per team by department\n",
    "df_cleaned['productivity_difference'] = df_cleaned['targeted_productivity'] - df_cleaned['actual_productivity']\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.boxplot(x='team', y='productivity_difference', hue='department', data=df_cleaned)\n",
    "plt.title(\"Productivity Difference vs Team by Department\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite standing out as having less productivity, team 6 of the finishing department and team 5 of the sewing department tend to reach their target productivity. On the other hand, higher productivity teams like team 1 of both departments tend to fall short of their target productivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the distribution of workers in each department to see if there are any imbalances in the number of workers in each department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of workers per team by department in table format\n",
    "workers_per_team = df_cleaned.groupby(['team', 'department'])['no_of_workers'].mean().reset_index()\n",
    "workers_per_team = workers_per_team.pivot(index='team', columns='department', values='no_of_workers')\n",
    "workers_per_team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from teams 6 and 12, the number of workers in each team is relatively balanced. So we look at other factors that may influence productivity such as incentives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#could the issue be related to incentives?\n",
    "df2 = df_cleaned.copy()\n",
    "# remove incentive outliers\n",
    "df2 = df2[df2['incentive'] < 700]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.boxplot(x='team', y='incentive', hue='department', data=df2)\n",
    "plt.title(\"incentive per team by department\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the finishing department rarely gets incentives while the sewing department gets incentives more frequently. Though it seems that incentives do not have a significant impact on whether or not a team reaches its target productivity. However, there does seem to be a slight positive correlation between incentives and productivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further dive into why some teams are unable to meet their target productivity, we look at the SMV of each team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perhaps the issue is in the smv\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.boxplot(x='team', y='smv', hue='department', data=df_cleaned)\n",
    "plt.title(\"SMV by team by department\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the SMV of the sewing department is generally higher than that of the finishing department. This explains the larger workforce in the sewing department as the tasks are more time-consuming. However, SMV does not seem to have a significant impact on productivity based on the graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V-C. Day of the Week and Quarter of the Month to Actual Productivity (Tean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Philippines, a company typically pays its workers at the end of the 2nd and 4th week of every month. With this, it can be speculated that the actual productivity of the workers will be affected when in the proximity of those times as they anticipate receiving their paychecks. Additionally, with the chosen representation, it will also be possible to analyze actual productivity as the month goes by."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant fields of the EDA:\n",
    "\n",
    "Days of the Week:\n",
    "- 'day_Monday'\n",
    "- 'day_Tuesday'\n",
    "- 'day_Wednesday'\n",
    "- 'day_Thursday'\n",
    "- 'day_Saturday'\n",
    "- 'day_Sunday'\n",
    "\n",
    "Quarters of the Month:\n",
    "- 'quarter_Quarter1'\n",
    "- 'quarter_Quarter2'\n",
    "- 'quarter_Quarter3'\n",
    "- 'quarter_Quarter4'\n",
    "- 'quarter_Quarter5'\n",
    "\n",
    "Actual Productivity:\n",
    "- 'actual_productivity'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seclude the relevant fields for the EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_quarter_productivity_df = df[['day_Monday', 'day_Tuesday', 'day_Wednesday', 'day_Thursday', 'day_Saturday', 'day_Sunday', 'quarter_Quarter1', 'quarter_Quarter2', 'quarter_Quarter3', 'quarter_Quarter4', 'quarter_Quarter5', 'actual_productivity']]\n",
    "\n",
    "day_quarter_productivity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the relevant fields into more easily processable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we are working with a copy of the DataFrame\n",
    "day_quarter_productivity_df = day_quarter_productivity_df.copy()\n",
    "\n",
    "# Make day_of_week and quarter columns\n",
    "day_quarter_productivity_df.loc[:, 'day_of_week'] = day_quarter_productivity_df[['day_Monday', 'day_Tuesday', 'day_Wednesday', 'day_Thursday', 'day_Saturday', 'day_Sunday']].idxmax(axis=1).str.split('_').str[1]\n",
    "day_quarter_productivity_df.loc[:, 'quarter'] = day_quarter_productivity_df[['quarter_Quarter1', 'quarter_Quarter2', 'quarter_Quarter3', 'quarter_Quarter4', 'quarter_Quarter5']].idxmax(axis=1).str.split('_').str[1]\n",
    "\n",
    "# Drop the one-hot encoded columns\n",
    "day_quarter_productivity_df = day_quarter_productivity_df.drop(['day_Monday', 'day_Tuesday', 'day_Wednesday', 'day_Thursday', 'day_Saturday', 'day_Sunday', 'quarter_Quarter1', 'quarter_Quarter2', 'quarter_Quarter3', 'quarter_Quarter4', 'quarter_Quarter5'], axis=1)\n",
    "\n",
    "day_quarter_productivity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Representation and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot Tables specialize in representing grouped data of many dimensions into a neat table format, where the rows and columns are the groupings and the cells are the featured values.\n",
    "As the EDA features 1 quantitative and 2 qualitative features, a Pivot Table should be the most fitting representation. \n",
    "This lets us create a table who's columns are the days of the week, rows are the quarters of the month, and cells are the means of the actual productivities grouped by the columns and rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table\n",
    "pivot_table = day_quarter_productivity_df.pivot_table(\n",
    "    values='actual_productivity', \n",
    "    index='quarter', \n",
    "    columns='day_of_week', \n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "# Sort the days of the week\n",
    "pivot_table = pivot_table[['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Saturday']]\n",
    "\n",
    "pivot_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better visualize the Pivot Table, a Heat Map can be made to highlight the trends of the data in each grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pivot table\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(pivot_table, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(6), pivot_table.columns)\n",
    "plt.yticks(np.arange(5), pivot_table.index)\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Quarter of the Month')\n",
    "plt.title('Average Productivity by Day of the Week and Quarter of the Month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Actual productivity of workers decreases as the month goes by, with the later half of the month being darker than the start. \n",
    "- The start of Quarter 3 (0.677855) and the end of Quarter 4 (0.654613) are especially dark, suggesting that the workers are less productive after receiving their 1st monthly paycheck and before receiving their 2nd monthly paycheck.\n",
    "- Quarter 5 is very productive as its colors are way brighter. However, this may be an outlier due to the only Quarter 5 days being 1/29/2015 (0.791633) and 1/31/2025 (0.854926)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V-D. Overtime in relation to Standard Minute Value and Number of Workers for each Department (Tan Ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering standard minute value was the amount of time it'd take to complete 1 work of an order, I was interested in seeing how overtime occurs in both departments based on the manpower they have, I split it into the two separate departments as the values for SMV varied greatly between departments, sensible enough as the finishing departments work such as trimming, button attachment, ironing, etc. are much less work (â€œFunctions of Finishing Department in Garment Industry,â€ 2015) when compared to the lengthy process of sewing the garments together.\n",
    "\n",
    "Relevant EDA fields:\n",
    "- Standard Minute Value (smv)\n",
    "- Overtime (over_time)\n",
    "- Number of Workers (no_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sewing_smv_nw_overtime_df = df[(df[\"department_sewing\"] == True)][['smv', 'no_of_workers', 'over_time']]\n",
    "\n",
    "finishing_smv_nw_overtime_df = df[(df[\"department_finishing\"] == True)][['smv', 'no_of_workers', 'over_time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sewing_smv_nw_overtime_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sewing_smv_nw_overtime_df = sewing_smv_nw_overtime_df.copy()\n",
    "\n",
    "finishing_smv_nw_overtime_df = finishing_smv_nw_overtime_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Representation and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D Scatterplots allow the relationship between variables to be observed in 3d space, considering that we have 3 quantitative variables this method of visualization should be fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = finishing_smv_nw_overtime_df['smv']\n",
    "y = finishing_smv_nw_overtime_df['no_of_workers']\n",
    "z = finishing_smv_nw_overtime_df['over_time']\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.scatter(x, y, z, c=z, cmap='viridis')\n",
    "\n",
    "ax.set_xlabel('Standard Minute Value')\n",
    "ax.set_ylabel('Number of Workers')\n",
    "ax.set_zlabel('Overtime')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sewing_smv_nw_overtime_df['smv']\n",
    "y = sewing_smv_nw_overtime_df['no_of_workers']\n",
    "z = sewing_smv_nw_overtime_df['over_time']\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.scatter(x, y, z, c=z, cmap='viridis')\n",
    "\n",
    "ax.set_xlabel('Standard Minute Value')\n",
    "ax.set_ylabel('Number of Workers')\n",
    "ax.set_zlabel('Overtime')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In both departments there are outliers that have an extreme amount of overtime.\n",
    "- The number of workers in the finishing department is lower than the number in the sewing department\n",
    "- Overtime has a tendency to become lower when the smv is low and there are more workers for the sewing department.\n",
    "- Despite average smv in the finishing department, the number of workers seem to correlate with increasing overtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Initial Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI-0. Train Test Set Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X` is the feature table and will all features except `actual_productivity`.\n",
    "\n",
    "`y` is the label of the feature table and will only contain `actual_productivity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['actual_productivity']).values\n",
    "y = df['actual_productivity'].values\n",
    "\n",
    "print('X ', X.shape)\n",
    "print('y ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the dataset into train and test sets, where `20%` of the data will be placed in the test set.\n",
    "\n",
    "Random state is set to `69` for uniformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the shape of the train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train', X_train.shape)\n",
    "print('y_train', y_train.shape)\n",
    "\n",
    "print('X_test', X_test.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display head and columns to check values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(X_train).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As sklearn's StandardScaler standardizes all feature columns regardless of datatype, need to exclude boolean columns from the standardization to maintain their values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all non-boolean columns for standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all columns in X_train that are not boolean\n",
    "non_boolean_columns = []\n",
    "for i in range(X_train.shape[1]):\n",
    "    if len(np.unique(X_train[:, i])) != 2:\n",
    "        non_boolean_columns.append(i)\n",
    "\n",
    "non_boolean_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the non-boolean columns based on X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale all columns that are not boolean\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scaler', StandardScaler(), non_boolean_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "ct.fit(X_train)\n",
    "\n",
    "X_train_scaled = ct.transform(X_train)\n",
    "X_test_scaled = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display head and columns of standardizes datasets to check values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(X_train_scaled).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to display MSE, RMSE, MAE, and R2 given a regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelPerformance(model, model_name:str, scaled:bool):\n",
    "    \n",
    "    # Predictions\n",
    "    if scaled:\n",
    "        y_train_pred = model.predict(X_train_scaled)\n",
    "        y_test_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "    else:\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Clip the values to be between 0 and 1\n",
    "    y_train_pred = np.clip(y_train_pred, 0, 1)\n",
    "    y_test_pred = np.clip(y_test_pred, 0, 1)\n",
    "\n",
    "    # MSE\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "    print(model_name + ' Train MSE:', train_mse)\n",
    "    print(model_name + ' Test MSE :', test_mse)\n",
    "    print('')\n",
    "    print(model_name + ' Train RMSE:', np.sqrt(train_mse))\n",
    "    print(model_name + ' Test RMSE :', np.sqrt(test_mse))\n",
    "    print('')\n",
    "    print(model_name + ' Train MAE:', mean_absolute_error(y_train, y_train_pred))\n",
    "    print(model_name + ' Test MAE :', mean_absolute_error(y_test, y_test_pred))\n",
    "    print('')\n",
    "    print(model_name + ' Train R^2:', r2_score(y_train, y_train_pred))\n",
    "    print(model_name + ' Test R^2 :', r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI-A. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Linear regression is a statistical and machine learning technique used to model the relationship between one or more independent variables (predictors) and a continuous dependent variable (target). It assumes a linear relationship between the predictors and the target, with the goal of finding the best-fitting line that minimizes the error between the predicted and actual values. Linear regression is simple to implement, computationally efficient, and interpretable, making it suitable for understanding relationships and making predictions. It performs well when the relationship between variables is approximately linear and is supported by statistical measures to evaluate the model's performance (Ross, 2010). However, it has limitations, such as sensitivity to outliers, reliance on assumptions like linearity and constant variance of errors, and difficulty handling multicollinearity or high-dimensional data. It may also struggle to capture complex or nonlinear relationships without additional transformations or feature engineering(Iqbal, 2020). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_linear_regression = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Hyperparameters for Linear Regression\n",
    "\n",
    "- fit_intercept: [True]\n",
    "- normalize: [False] (deprecated in version 1.0, removed in 1.2)\n",
    "- copy_X: [True]\n",
    "- n_jobs: [None]\n",
    "\n",
    "by setting normalize as true, Linear regression can benefit from feature scaling when the input variables have different scales (e.g., one feature is measured in meters and another in kilometers). It is important to note that Standard Scaling should be used instead of the old normalize hyperparameter in scikit linear regression.\n",
    "\n",
    "Copy_X controls whether the model should make a copy of the feature matrix ð‘‹ before fitting the model. By default, it ensures that the original data is not modified during the fitting process. If set to False, it can save memory if you're working with large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_linear_regression.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getModelPerformance(initial_linear_regression, 'Linear Regression', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI-B. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees have been used for classification and regression problems, however it faces the issue of being prone to bias and overfitting due to the nature of the algorithm. Therefore to remedy this we have decided to make use of the Random Forest model which is an ensemble learning method that makes use of decision trees.\n",
    "\n",
    "The way random forests are able to improve its accuracy greatly over decision trees is due to various techniques with the first being bootstrapping to create a new dataset which can sample duplicates of the original dataset, and instead of considering all features of the dataset per split (i.e. all 24 columns for our dataset) it only considers a random subset of features (though the default values for sklearn consider all features), these two techniques in particular are what allow the individual decision trees to have little correlation with one another which helps with preventing overfitting.\n",
    "\n",
    "Aggregating the decision of the multiple trees together leads to bagging, in the case of classification a majority vote is used to classify the unknown instance, whereas in regression the outputs of the trees will be averaged to get the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_rf_regressor = RandomForestRegressor(random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- n_estimators: [100]\n",
    "- max_depth: [None]\n",
    "- min_samples_split: [2]\n",
    "- min_samples_leaf: [1]\n",
    "- min_weight_fraction_leaf: [0.0]\n",
    "- max_features: [1.0]\n",
    "- max_leaf_nodes: [None]\n",
    "- min_impurity_decrease: [0.0]\n",
    "- bootstrap: [True]\n",
    "- oob_score: [False]\n",
    "- n_jobs: [None]\n",
    "- random_state: [69]\n",
    "- verbose: [1]\n",
    "- warm_start: [False]\n",
    "- ccp_alpha: [0.0]\n",
    "- max_samples: [None]\n",
    "- monotonic_cst: [None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters above are all of the parameters that can be modified when creating the RandomForestRegressor(), however the main hyperparameters are:\n",
    "- n_estimators\n",
    "- max_depth\n",
    "- max_features\n",
    "\n",
    "Where n_estimators refers to the total number of trees that will be created by the model.\n",
    "\n",
    "Where max_depth refers to the longest path that a model can create, which in its default state will create trees that go on until they are pure or until they reach min_samples_split.\n",
    "\n",
    "Where max_features refers to the number of features to randomly sample at each split, which at default considers all features at each split (similar to a regular decision tree).\n",
    "\n",
    "Some of these default values such as the n_estimators came from the paper that originally postulated the concept of the random forest (Breiman, 2001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_rf_regressor.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getModelPerformance(initial_rf_regressor, 'Initial Random Forest Model', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI-C. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks are a robust tool for predicting values due to their ability to capture complex, non-linear relationships between input features. \n",
    "With columns ranging from operational metrics like SMV, WIP, and over_time, to dynamic factors such as idle_time and no_of_style_change, Neural Networks can effectively process and learn from the intricate patterns and interactions among these variables. \n",
    "Additionally, their adaptability to various data scales and distributions makes them particularly suited to handle the diverse nature of the dataset, potentially leading to more accurate and insightful predictions of actual productivity. This capacity to model complex dependencies sets Neural Networks apart as a powerful predictive tool in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the Neural Network model for the dataset, scikit-learn's Multi-layer Perceptron Regressor (MLP Regressor) will be used. \n",
    "It is a straightforward implementation of the supervised Neural Network model intended for simple use cases and learning, and it is complete with interconnected nodes, forwardpropagation, backpropagation, weight adjustment, and more.\n",
    "Moreover, the MLP Regressor also comes with various hyperparameters that can be tuned to optimize performance and achieve high accuracy in predictive modeling tasks. (1.17. Neural Network Models (Supervised), n.d.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCRAPPED: Model would exploit clip and always go on top\n",
    "\n",
    "Additionally, as the output of the regressor should only be between 0 and 1, to prevent it from having values beyond those, a custom MLPRegressor was made with with predictions being clipped at 0 and 1 before being returned.\n",
    "By doing so, the model can train without needing to worry about going above or below the value ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ClippedMLPRegressor(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "#     def __init__(self, **kwargs):\n",
    "#         self.model = MLPRegressor(**kwargs)\n",
    "#         self.max_iter = self.model.max_iter\n",
    "    \n",
    "#     def fit(self, X, y):\n",
    "#         self.model.fit(X, y)\n",
    "#         return self\n",
    "    \n",
    "#     def partial_fit(self, X, y):\n",
    "#         self.model.partial_fit(X, y)\n",
    "#         return self\n",
    "    \n",
    "#     def get_params(self, deep: bool = True) -> dict:\n",
    "#         return self.model.get_params(deep)\n",
    "    \n",
    "#     def predict(self, X):\n",
    "#         predictions = self.model.predict(X)\n",
    "#         return np.clip(predictions, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLPRegressor model\n",
    "initial_nn_model = MLPRegressor(random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main hyperparameters:\n",
    "- Neurons and hidden layers: [100]\n",
    "- Solver and Optimizer: ADAM\n",
    "- Activation Function: ReLU\n",
    "- Max Iterations: 200\n",
    "- Initial Learning Rate: 0.001\n",
    "- Learning Rate Change: constant\n",
    "- Regularization Constant: 0.0001\n",
    "- Random Sate: 69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters used for the initial model training are the default parameters of the MLPRegressor model. \n",
    "This is to set a base standard to which the model can improve upon when tuning. \n",
    "In general, it can be noted that the default parameters themselves are already sufficient for most use cases, already including advanced Neural Network optimizations like ADAM and ReLU.\n",
    "\n",
    "Out of all the hyperparameters, the most significant is that there is only 1 hidden layer with 100 neurons. \n",
    "As the initial model, it is generally better to have a simple model with lesser neurons and hidden layers, then slowly increase the number over iterations.\n",
    "This way, it is possible to see how the model improves with regards to the hyperparameters (Karpathy, 2019; Dutta, 2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_nn_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the model trains and improves, the training of the initial MLPRegressor will use partial_fit to fit it per epoch, and only stop when reach max iterations.\n",
    "After each epoch, the error metrics for both train and test sets will be stored for evaluation later on, and at every 5th epoch, they will be displayed.\n",
    "As the MLP Regressor model is sensitive to feature scaling, use the scaled versions of X_train and X_test to train the model. (1.17. Neural Network Models (Supervised), n.d.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use partial_fit to train the model and get the error metrics for each epoch\n",
    "\n",
    "nn_epoch_num = 1\n",
    "nn_epoch_train_mse = []\n",
    "nn_epoch_train_mae = []\n",
    "nn_epoch_r2 = []\n",
    "nn_epoch_test_mse = []\n",
    "nn_epoch_test_mae = []\n",
    "nn_epoch_test_r2 = []\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # Train the model for one epoch\n",
    "    initial_nn_model.partial_fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict the training set\n",
    "    y_train_pred = initial_nn_model.predict(X_train_scaled)\n",
    "    y_test_pred = initial_nn_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Clip the values to be between 0 and 1\n",
    "    y_train_pred = np.clip(y_train_pred, 0, 1)\n",
    "    y_test_pred = np.clip(y_test_pred, 0, 1)\n",
    "    \n",
    "    # Calculate the error metrics\n",
    "    train_rmse = mean_squared_error(y_train, y_train_pred)\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_rmse = mean_squared_error(y_test, y_test_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Append the mean squared error to the list\n",
    "    nn_epoch_train_mse.append(train_rmse)\n",
    "    nn_epoch_train_mae.append(train_mae)\n",
    "    nn_epoch_r2.append(train_r2)\n",
    "    nn_epoch_test_mse.append(test_rmse)\n",
    "    nn_epoch_test_mae.append(test_mae)\n",
    "    nn_epoch_test_r2.append(test_r2)\n",
    "    \n",
    "    # Print the epoch number and error metrics for every 10th epoch\n",
    "    if nn_epoch_num % 10 == 0:\n",
    "        print(f\"Epoch: {nn_epoch_num}\")\n",
    "        print(f\"\\tTrain - RMSE: {np.sqrt(train_rmse)}, MAE: {train_mae}, R^2: {train_r2}\")\n",
    "        print(f\"\\tTest  - RMSE: {np.sqrt(test_rmse)}, MAE: {test_mae}, R^2: {test_r2}\")\n",
    "    \n",
    "    # Break the loop if reach max_iter\n",
    "    if nn_epoch_num == initial_nn_model.max_iter:\n",
    "        break\n",
    "    else:\n",
    "        # Increment the epoch number\n",
    "        nn_epoch_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model has been trained, display its error metrics for both train and test sets to show its performance. The analysis for the errors will be featured in the Error Analysis section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getModelPerformance(initial_nn_model, 'Initial Neural Network', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII. Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII-0. Plotting Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPredToActualTable(y_test, y_pred, show_instances, is_test):\n",
    "    string = 'Test' if is_test else 'Train'\n",
    "    return pd.DataFrame({'Predicted Productivity': y_pred[:show_instances], string + ' Dataset Productivity': y_test[:show_instances]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTrainingTestErrors(train_rmse, train_mae, test_rmse, test_mae):\n",
    "    # Plot the training and testing errors\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train_rmse, label='Train RMSE', color='blue', linestyle=\"-\")\n",
    "    plt.plot(train_mae, label='Train MAE', color='blue', linestyle=\"--\")\n",
    "    plt.plot(test_rmse, label='Test RMSE', color='red', linestyle=\"-\")\n",
    "    plt.plot(test_mae, label='Test MAE', color='red', linestyle=\"--\")\n",
    "    plt.title('Training and Testing Errors')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTrainingTestR2(train_r2, test_r2):\n",
    "    # Plot the training and testing R^2\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train_r2, label='Train R^2', color='blue')\n",
    "    plt.plot(test_r2, label='Test R^2', color='red')\n",
    "    plt.title('Training and Testing R^2')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('R^2')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotProductivityError(y_test, y_pred, model_name, is_test):\n",
    "    string = 'Test' if is_test else 'Train'\n",
    "    indices = np.arange(len(y_pred))\n",
    "\n",
    "    plt.figure(figsize=(25, 6))\n",
    "    plt.plot(indices, y_pred, label=model_name+' Predicted Productivity', marker='', linestyle='-', color='blue', alpha=0.7)\n",
    "    plt.plot(indices, y_test, label=string + ' Dataset Productivity', marker='', linestyle='--', color='red', alpha=0.7)\n",
    "\n",
    "    plt.title(\"Comparison of Predicted vs \" + string + \" Dataset Productivity for \" + model_name)\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"Productivity\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotProductivityDistribution(y_test, y_pred, model_name, is_test):\n",
    "    string = 'Test' if is_test else 'Train'\n",
    "    min_value = min(np.min(y_test), np.min(y_pred))\n",
    "    max_value = max(np.max(y_test), np.max(y_pred))\n",
    "    bin_edges = np.linspace(min_value, max_value, 50)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    sns.histplot(y_pred, kde=True, color='blue', label=model_name+' Predicted Productivity', stat='percent', bins=bin_edges, alpha=0.6)\n",
    "    sns.histplot(y_test, kde=True, color='red', label=string + ' Dataset Productivity', stat='percent', bins=bin_edges, alpha=0.6)\n",
    "\n",
    "    plt.title(\"Distribution of Actual Productivity for \" + model_name + \" in \" + string + \" Dataset\")\n",
    "    plt.xlabel(\"Actual Productivity\")\n",
    "    plt.ylabel(\"Frequency (%)\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII-A. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the initial performance scores of the linear regression model, the model is relatively performing okay but with many instances of overfitting. The model was able to capture the various trends in the data but extreme deviations observed imply that the model struggles with noise in the dataset.\n",
    "\n",
    "The presence of consistent variability across both graphs suggest that the model may further be improved using other techniques which may include regularization and further feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getModelPerformance(initial_linear_regression, 'Initial Linear Regression', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_train_pred = initial_linear_regression.predict(X_train_scaled)\n",
    "lr_test_pred = initial_linear_regression.predict(X_test_scaled)\n",
    "\n",
    "print(\"Predicted - Train:\\n\")\n",
    "print(showPredToActualTable(y_train, lr_train_pred, 20, False))\n",
    "print(\"\\n\")\n",
    "print(\"Predicted - Test:\\n\")\n",
    "print(showPredToActualTable(y_test, lr_test_pred, 20, False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotProductivityError(y_train, lr_train_pred, 'Initial Linear Regression (Train)', True)\n",
    "plotProductivityError(y_test, lr_test_pred, 'Initial Linear Regression(Test)', True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII-B. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seen below is the performance of the initial random forest regressor, where the performance of it is good for the training set, but it falls off greatly in the test set with its RMSE being much higher in the test set, and its R^2 score much lower where it matches only around 32% of the ground truth properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getModelPerformance(initial_rf_regressor, 'Initial Random Forest', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train_pred = initial_rf_regressor.predict(X_train)\n",
    "rf_test_pred = initial_rf_regressor.predict(X_test)\n",
    "\n",
    "print(\"Predicted - Train:\\n\")\n",
    "print(showPredToActualTable(y_train, rf_train_pred, 20, False))\n",
    "print(\"\\n\")\n",
    "print(\"Predicted - Test:\\n\")\n",
    "print(showPredToActualTable(y_test, rf_test_pred, 20, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotProductivityError(y_train, rf_train_pred, 'Initial Random Forest (Train)', False)\n",
    "plotProductivityError(y_test, rf_test_pred, 'Initial Random Forest(Test)', False)\n",
    "\n",
    "plotProductivityDistribution(y_train, rf_train_pred, 'Initial Neural Network (Train)', False)\n",
    "plotProductivityDistribution(y_test, rf_test_pred, 'Initial Neural Network (Test)', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII-C. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the performance of the initial neural network model\n",
    "getModelPerformance(initial_nn_model, 'Initial Neural Network', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shown above are the final performance metrics of the Initial Neural Network after training.\n",
    "\n",
    "For the train error metrics, it is not surprising that they are very low as the model is trained with it. Meanwhile for the test error metrics, given that it only used the default parameters for the MLP Regressor, its error metrics are already very low.\n",
    "\n",
    "As the test RMSE and MAE are around 0.1~, it suggests that the the average error of the model's predicted value is +- 0.1~ from the test set's actual_productivity value.\n",
    "However, the test R^2 error is 0.14, meaning that the model's predicted values only match around 14% of all the test set's actual_productivity values.\n",
    "These values mean that the average error of the model to the test set is low, but it doesn't actually fit majority of the data trend well as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted productivity for the train and test dataset\n",
    "nn_y_train_pred = initial_nn_model.predict(X_train_scaled)\n",
    "nn_y_test_pred = initial_nn_model.predict(X_test_scaled)\n",
    "\n",
    "# Scale the predicted values back to 0 and 1\n",
    "nn_y_train_pred = np.clip(nn_y_train_pred, 0, 1)\n",
    "nn_y_test_pred = np.clip(nn_y_test_pred, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first 20 instances of the predicted and actual productivity for the train and test dataset\n",
    "print(\"Predicted - Train:\\n\")\n",
    "print(showPredToActualTable(y_train, nn_y_train_pred, 20, False))\n",
    "print(\"\\n\")\n",
    "print(\"Predicted - Test:\\n\")\n",
    "print(showPredToActualTable(y_test, nn_y_test_pred, 20, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shown above are a subset of the predictions for the train and test sets of the Initial Neural Network.\n",
    "This way we can observe the raw predicted values and how they compare to the actual ones.\n",
    "\n",
    "In general, one can see that many of the predicted values for the train set are pretty close to the actual values, corresponding to the low train error metrics. \n",
    "However, going into the test set, there are usually large variances between the values, also corresponding to the test error metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTrainingTestErrors(np.sqrt(nn_epoch_train_mse), nn_epoch_train_mae, np.sqrt(nn_epoch_test_mse), nn_epoch_test_mae)\n",
    "plotTrainingTestR2(nn_epoch_r2, nn_epoch_test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shown above are the test and train error metrics of the model as it is trained per epoch.\n",
    "\n",
    "For the train and test RMSE and MAE, they both followed the standard graph for decreasing their error values, suggesting they trained properly. \n",
    "But it can be noted that there is a small local-minima for the test set at around the 5th epoch that the model was able to overcome.\n",
    "It can also be seen that the train error metrics are significantly lower than the test ones, highlighting how the model fits more to the train set due to being trained with it.\n",
    "The same trend can also be seen with the train and test R^2 scores, with the train being higher than the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotProductivityError(y_train, nn_y_train_pred, 'Initial Neural Network (Train)', False)\n",
    "plotProductivityError(y_test, nn_y_test_pred, 'Initial Neural Network (Test)', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shown above are the comparisons of the predicted and actual productivity values of the Initial Neural Network to the train and test set.\n",
    "\n",
    "Looking into the train set's graph, it can be seen how the model's predictions try its best to match those of the train set's actual productivity values. \n",
    "It was able to capture a certain trend of the data, thus matching it well in many of the points (although, it is quite hard to see).\n",
    "However, it struggles to match the large variance of the higher values.\n",
    "\n",
    "Using the model for the test set, one can see how the values don't match as well in the test set's graph.\n",
    "Although it was able to follow a general trend, there were many cases where it failed to follow all the points.\n",
    "Another observation was how the model's predictions tend to be more conservative as compared to the predicted values, opting to give less-varied productivity values.\n",
    "Similar to the train set's graph, this makes it struggle to match the large variance of higher values; this may explain why the R^2 score was pretty low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotProductivityDistribution(y_train, nn_y_train_pred, 'Initial Neural Network (Train)', False)\n",
    "plotProductivityDistribution(y_test, nn_y_test_pred, 'Initial Neural Network (Test)', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shown above are the histograms of the predicted and actual productivity values of the Initial Neural Network to the train and test set.\n",
    "This visualizes the same data in another perspective to see more trends on the data.\n",
    "\n",
    "Right out the bat, one can see how the data for the actual productivity is generally skewed to certain productivity scores, as shown by the distribution of the histogram.\n",
    "Meanwhile, the predictions given by the model are a lot more conservative and spreads out its values.\n",
    "These confirm the suspicion on the previous analysis. \n",
    "\n",
    "This unbalanced skew makes it difficult for the model to consistently predict the actual values of the dataset, resulting to worse error metrics, despite the trend lines in the graph being relatively similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIII. Improving Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII-A. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve the performance of the linear regression, we used 2 ways of tuning the hyperparameters:\n",
    "\n",
    "    1. Linear Regression with Lasso Regularization\n",
    "    2. Stochastic Gradient Descent with Grid Search\n",
    "\n",
    "Using both methods provides a broader range of optimization strategies, improves robustness, and ensures better adaptability to different data characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Existing Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both models that are trained are stored in an external file, which is checked and loaded by the following cell. This is to avoid retraining the model, even if the models are trained relatively quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lasso_gscv_model = None\n",
    "\n",
    "\n",
    "sgd_lr_gscv_model = None\n",
    "\n",
    "try:\n",
    "    pipeline_lasso_gscv_model = joblib.load('pipeline_lasso_gscv.pkl')\n",
    "except:\n",
    "    pipeline_lasso_gscv_model = None\n",
    "\n",
    "try: \n",
    "    sgd_lr_gscv_model = joblib.load('sgd_lr_gscv.pkl')\n",
    "except:\n",
    "    sgd_lr_gscv_model = None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Pipeline using Lasso Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Regularization was employed to reduce overfitting by penalizing large coefficients, resulting in a simpler model. By doing regularization, we hope to see if the model can generalize unseen and noise in the data better. Grid search is done for the alpha hyperparameter value to see what regularization value would yield the best performance. The closer the alpha value is to 0, the more similar this is to a simple linear regression without regularization (Tibshirani, 1996)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lasso_gscv_model = None\n",
    "\n",
    "try:\n",
    "    pipeline_lasso_gscv_model = joblib.load('pipeline_lasso_gscv.pkl')\n",
    "except:\n",
    "    pipeline_lasso_gscv_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_gcsv_hyperparameters = None\n",
    "\n",
    "if pipeline_lasso_gscv_model is None:\n",
    "    lasso_gcsv_hyperparameters = {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pipeline_lasso_gscv_model is None:\n",
    "    pipeline_lasso_gscv_model = Lasso()\n",
    "    # implement grid search\n",
    "    pipeline_lasso_gscv_model = GridSearchCV(pipeline_lasso_gscv_model, lasso_gcsv_hyperparameters, cv=5, verbose=1, n_jobs=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lasso_gscv_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(pipeline_lasso_gscv_model, 'pipeline_lasso_gscv.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lasso_gscv_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lasso_gscv_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getModelPerformance(pipeline_lasso_gscv_model, 'Pipeline Lasso Regression', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent with Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent is an optimization algorithm used to minimize a cost function, which in our case is the mean squared error, by iteratively updating the model's parameters.\n",
    "\n",
    "The randomness introduced by updating parameters using a single or small batch of samples reduces the risk for overfitting. By introducing randomness in the gradient updates, the noise prevents the model from converging too precisely to the training data's local minima (Jupudi, 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_lr_gscv_hyperparameters = None\n",
    "\n",
    "if sgd_lr_gscv_model is None:\n",
    "    sgd_lr_gscv_hyperparameters = {\n",
    "        'fit_intercept': [True, False],\n",
    "        'max_iter': [200, 400, 600, 800, 1000],\n",
    "        'learning_rate': ['adaptive', 'constant', 'invscaling'],\n",
    "        'eta0': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
    "        'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune| the model using `GridSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sgd_lr_gscv_model is None:\n",
    "    sgd_lr_gscv_model = SGDRegressor(random_state=random_seed)\n",
    "    sgd_lr_gscv_model = GridSearchCV(sgd_lr_gscv_model, sgd_lr_gscv_hyperparameters, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_lr_gscv_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(sgd_lr_gscv_model, 'sgd_lr_gscv.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the best estimator of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_lr_gscv_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the performance of the best estimator of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getModelPerformance(sgd_lr_gscv_model.best_estimator_, 'SGD with Grid Search', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII-B. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An issue that random forest models face is the more minimal effect of tuning compared to other models such as Support Vector Machines or Neural Networks, as something like n_estimators is dissimilar from other hyperparameters as it is almost always strictly better to go for higher values but only up to the point that tradeoff between performance and training time are not too great; while one of the hyperparameters that generally seem to have a greater effect is the number of random features considered at each split which in our case would be the max_features parameter(Probst et al., 2019). \n",
    "\n",
    "They've also suggested the usage of grid search, random search, and other tuning strategies to find optimal values for the hyper parameters, in our case we plan to implement random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load the model\n",
    "tuned_rf_model = None\n",
    "try:\n",
    "    tuned_rf_model = joblib.load('rf_rscv.pkl')\n",
    "except:\n",
    "    tuned_rf_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_rf_regressor = RandomForestRegressor(random_state=random_seed) #8, 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that the tradeoff for n_estimators is linear, its value will be continually increased until there has been a noticeable tradeoff on time, while for max depth the range of values has no particular study supporting the value range, however it should suffice considering the number of features in the dataset and the max depth exhibited by previous testing iterations. Another parameter that is important to tune is the max_features parameter as it controls the number of randomly selected features when splitting allowing the algorithm to make trees with less correlation leading to less overfitting.\n",
    "\n",
    "The criterion for splitting used by the random forest may be changed from the default (mean squared error) to absolute error which uses MAE, and friedman_mse which uses the friedman_score which calculates the impurity of the split alongside the meansquared error. \n",
    "\n",
    "Although it is inherently part of the algorithm according to Breiman (2001), there are also some cases where the algorithm performs better when the bootstrapping does not occur, meaning that there are no duplicate instances in the subset created for the training of the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_hyperparameters = [\n",
    "        {\n",
    "            'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "            'criterion': ['squared_error', 'absolute_error', 'friedman_mse'],\n",
    "            'max_depth': [40, 50, 60, 70, 80, 90, 100],      \n",
    "            'min_samples_leaf': [1, 3, 5],\n",
    "            'max_features': [6, 12, 14, 18, 24,\"sqrt\", \"log2\"],\n",
    "            'bootstrap': [True, False]\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tuned_rf_model is None:\n",
    "    tuned_rf_model = RandomizedSearchCV(initial_rf_regressor, rf_hyperparameters, n_iter=500, cv=5, n_jobs=-1, random_state=random_seed)\n",
    "    tuned_rf_model.fit(X_train, y_train)\n",
    "    joblib.dump(tuned_rf_model, 'rf_rscv.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tuned_rf_model.cv_results_).sort_values('rank_test_score', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_rf_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what can be inferred in the performance of the top 10 parameters values for the best parameters are as follows:\n",
    "- n_estimators = [100, 200, 300, 500, 700, 800, 900]\n",
    "- min_samples_leaf = [1]\n",
    "- max_features = [6, 12, 14]\n",
    "- max_depth = [50, 60, 70, 80, 90]\n",
    "- criterion = ['squared error', 'absolute_error']\n",
    "- bootstrap = [True, False]\n",
    "\n",
    "Most of the high-performing parameters have n_estimators greater than the default value of 100 supporting the idea of convergence becoming better as the number of decision trees increase, whereas the min_samples_leaf remains similar to the default value where the final leaf nodes are pure. Another thing to note is that the value for maximum features tended towards lower values as a good chunk of the top 10 used max_features = 6, which allowed for randomizing features more. The splitting criterion was mainly split between mean squared error and mean absolute error amongst the top 10, where friedman score with MSE and poisson regression were not used as much, and the reason why mean absolute error was chosen over mean square error is likely due to the presence of the outliers as MAE is more robust against outliers. Finally, bootstrapping remained true for a majority of the models.\n",
    "\n",
    "One thing to note is that the model has been able to generalize better and increased its performance on the test set with the hyper parameter tuning, even if the increase is only by 2% according to the R^2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getModelPerformance(tuned_rf_model, 'Initial Random Forest Model', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII-C. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the Initial Neural Network model and evaluating its performance, now it is time to create an improved model by using hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the hyperparameter tuning and training of Neural Networks takes a while to complete (40 minutes in this case), it was more economical to export and save the model for later use to prevent needing to retrain a new model every time the notebook is run.\n",
    "\n",
    "The code below checks whether there already exists a Tuned Neural Network in the folder to use for testing and evaluation.\n",
    "If there is already a saved model, it will not run the hyperparameter tuning anymore and will use the existing model.\n",
    "Otherwise, it will train and tune a model, then save it into the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load the model\n",
    "tuned_nn_model = None\n",
    "try:\n",
    "    tuned_nn_model = joblib.load('nn_rscv.pkl')\n",
    "except:\n",
    "    tuned_nn_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to get all a list of all possible combinations of hidden layers given the minimum layers, maximum layers, and neuron options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all possible hidden layer sizes combinations\n",
    "def get_hidden_layer_sizes(min_layers, max_layers, neuron_options):\n",
    "    hidden_layer_sizes = []\n",
    "    for num_layers in range(min_layers, max_layers + 1):\n",
    "        for combination in itertools.product(neuron_options, repeat=num_layers):\n",
    "            hidden_layer_sizes.append(combination)\n",
    "    return hidden_layer_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the hyperparameter options, most of them were chosen with the basis being that they are the standard and most used.\n",
    "This goes for the Initial Learning Rate and Regularization Constant being decimals of 1.\n",
    "As for the max iterations, several standard values were included for the model to select the best number of iterations while preventing under and over fitting.\n",
    "\n",
    "Meanwhile, as for the optimizer and activation function, they can be selected from those that are available within the MLPRegressor model.\n",
    "In general, the most performant of the available options were selected, such as selecting ADAM over Stochastic Gradient Descent, TanH and ReLU over Logistic, and Adaptive over Constant learning rate change.\n",
    "\n",
    "Lastly, the most significant of the hyperparameters is the number of hidden layers and neurons per layer. \n",
    "Although there is no definite answer for these parameters, a good starting point for simple problems is to have 1-2 hidden layers with 2/3 of the input size number for the neurons per layer (An, 2022).\n",
    "However, the current problem is very intricate and complex with its values, so the simple case may not very feasible.\n",
    "Additionally, as Python itself can output Memory Errors if a list is too big, certain values had to be limited for the model.\n",
    "In the end, after multiple tunning iterations, it was found that 10 hidden layers with a combination of 50, 100, 250, and 500 neurons were the most performant in the hyperparameter tuning.\n",
    "This was chosen to highlight the complexity of the features by adding multiple hidden layers, while also distributing neuron sizes to balance out the neurons for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tunning Options:\n",
    "- Neurons and hidden layers: 10 layers with a combination of [50, 100, 250, 500] neurons\n",
    "- Solver and Optimizer: ADAM\n",
    "- Activation Function: [TanH, ReLU]\n",
    "- Max Iterations: [200, 400, 600, 800, 1000]\n",
    "- Initial Learning Rate: [0.1, 0.01, 0.001, 0.0001]\n",
    "- Learning Rate Change: adaptive\n",
    "- Regularization Constant: [0.0001, 0.001, 0.01, 0.1]\n",
    "- Random Sate: 69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_hyperparameters = None\n",
    "\n",
    "# Define hyperparameters\n",
    "if tuned_nn_model is None:\n",
    "    \n",
    "    # Define hidden layer sizes\n",
    "    neuron_options = [50, 100, 250, 500]\n",
    "    min_layers = 10\n",
    "    max_layers = 10\n",
    "    \n",
    "    nn_hyperparameters = [\n",
    "        {\n",
    "            'hidden_layer_sizes': get_hidden_layer_sizes(min_layers, max_layers, neuron_options),\n",
    "            'solver': ['adam'],\n",
    "            'activation': ['tanh', 'relu'],\n",
    "            'max_iter': [200, 400, 600, 800, 1000],\n",
    "            'learning_rate_init': [0.1, 0.01, 0.001, 0.0001],\n",
    "            'learning_rate': ['adaptive'],\n",
    "            'alpha': [0.0001, 0.001, 0.01, 0.1]\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the actual hyperparameter tuning technique, the RandomizedSearchCV technique was used.\n",
    "\n",
    "RandomizedSearchCV is a method for tuning the hyperparameters of a model by randomly selecting values rather than using a fixed grid of parameters, like GridSearchCV. \n",
    "In each iteration, it tests a different set of hyperparameters and records how well the model performs by using Cross-Validation to separate the train set into train and validation and evaluate them. \n",
    "After multiple iterations, it identifies the combination that yields the best results. \n",
    "This random selection approach reduces unnecessary calculations compared to testing every possible combination systematically and can generally yield similar results (GeeksforGeeks, 2023).\n",
    "\n",
    "To tune the hyperparameters faster, the n_jobs parameter was set to -1, which means that the model will use all available CPU cores to tune the model concurrently, thus saving time.\n",
    "The n_iter parameter was also set to 100. \n",
    "Although this increases training time, it gives the RandomizedSearchCV more of a chance to stumble across a more accurate combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized Search to find the best hyperparameters and Train the model\n",
    "if tuned_nn_model is None:\n",
    "    \n",
    "    tuned_nn_regressor = MLPRegressor(random_state=random_seed)\n",
    "    tuned_nn_model = RandomizedSearchCV(tuned_nn_regressor, nn_hyperparameters, n_iter=100, cv=5, n_jobs=-1, random_state=random_seed)\n",
    "    tuned_nn_model.fit(X_train_scaled, y_train)\n",
    "    joblib.dump(tuned_nn_model, 'nn_rscv.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the top 10 best performing hyperparameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tuned_nn_model.cv_results_).sort_values('rank_test_score', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the top performing hyperparameter configurations, there is a clear gap between the mean test score performance of the top 5 combinations and the rest, with all of them being almost 40% accurate to the validation sets.\n",
    "Of the top 5 combinations, there is a trend for them to have smaller learning rates and regularization constants.\n",
    "This suggest that they might overfit to the training data due to training with them for many iterations and having little to combat high variance.\n",
    "This may also explain why they might have achieved higher mean test scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the best hyperparameters chosen after running the hyperparameter tunning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most Performant Hyperparameter Values:\n",
    "- Neurons and hidden layers: 10 layers with [500, 250, 250, 100, 500, 500, 250, 500, 250, 50] neurons\n",
    "- Solver and Optimizer: ADAM\n",
    "- Activation Function: TanH\n",
    "- Max Iterations: 1000\n",
    "- Initial Learning Rate: 0.0001\n",
    "- Learning Rate Change: adaptive\n",
    "- Regularization Constant: 0.0001\n",
    "- Random Sate: 69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "tuned_nn_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the most performant hyperparameters, it can be implied that the Tuned Neural Network took a slower approach to its training; as highlighted by its very low initial learning rate and very high max iterations.\n",
    "In most cases, one may think that there is a high chance that the model would overfit to the training set, as it was fitted to it through many iterations and that its regularization constant is very low.\n",
    "Despite this, it still learned the trend of the data and was able to get pretty good predictions to the productivity values (to which will be discussed later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the hyperparameters have been tuned and the model has been trained, display its error metrics for both train and test sets to show its performance. The analysis for the errors will be featured in the Model Performance Summary section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the performance of the model\n",
    "getModelPerformance(tuned_nn_model, 'Tuned Neural Network', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IX. Model Performance Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IX-A. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shown below are the performances of two models, i.e., Linear Regression with Lasso Regularization and Stochastic Gradient Descent. It is evidently observed that the two models perform with no significant improvement from an ordinary linear regression model. This may be further attributed to the high variability in a very small dataset. For both training and test dataset, there is observed overfitting, despite the model being able to capture the trends in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getModelPerformance(initial_linear_regression, 'Initial Linear Regression', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getModelPerformance(pipeline_lasso_gscv_model.best_estimator_, 'Linear Regression with Lasso Regularization', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lasso_train_pred = pipeline_lasso_gscv_model.predict(X_train_scaled)\n",
    "pipeline_lasso_test_pred = pipeline_lasso_gscv_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Linear Regression with Lasso Regularization\")\n",
    "print(\"Predicted - Train:\\n\")\n",
    "print(showPredToActualTable(y_train, pipeline_lasso_train_pred, 20, False))\n",
    "print(\"\\n\")\n",
    "print(\"Predicted - Test:\\n\")\n",
    "print(showPredToActualTable(y_test, pipeline_lasso_test_pred, 20, True))\n",
    "print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotProductivityError(y_train, pipeline_lasso_train_pred, 'Lasso Linear Regression (Train)', True)\n",
    "plotProductivityError(y_test, pipeline_lasso_test_pred, 'Lasso Linear Regression (Test)', True)\n",
    "\n",
    "plotProductivityDistribution(y_train, pipeline_lasso_train_pred, 'Lasso Linear Regression (Train)', True)\n",
    "plotProductivityDistribution(y_test, pipeline_lasso_test_pred, 'Lasso Linear Regression (Test)', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is shown that compared to the simple linear regression model, there is not much improvement in performance observed. For Lasso Regularization, the regularization parameter alpha was equal to 0.001, yielded the best performance. As the alpha value is too close to zero, the L1 penalty becomes negligible. In this scenario, Lasso will behave like a simple linear regression as there is no significant penalty to shrink coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getModelPerformance(sgd_lr_gscv_model.best_estimator_, 'SGD with Grid Search', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_lr_gscv_train_pred = sgd_lr_gscv_model.predict(X_train_scaled)\n",
    "sgd_lr_gscv_test_pred = sgd_lr_gscv_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"SGD - Grid Search\")\n",
    "print(\"Predicted - Train:\\n\")\n",
    "print(showPredToActualTable(y_train, sgd_lr_gscv_train_pred, 20, False))\n",
    "print(\"\\n\")\n",
    "print(\"Predicted - Test:\\n\")\n",
    "print(showPredToActualTable(y_test, sgd_lr_gscv_test_pred, 20, False))\n",
    "print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotProductivityError(y_train, sgd_lr_gscv_train_pred, 'SGD with Grid Search (Train)', True)\n",
    "plotProductivityError(y_test, sgd_lr_gscv_test_pred, 'SGD with Grid Search (Test)', True)\n",
    "\n",
    "plotProductivityDistribution(y_train, sgd_lr_gscv_train_pred, 'SGD with Grid Search (Train)', True)\n",
    "plotProductivityDistribution(y_test, sgd_lr_gscv_test_pred,  'SGD with Grid Search (Test)', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we used SGD without regularization, it simply minimized the same cost function as a simple linear regression model, which is the mean squared error in this case. Hence, SGD will yield similar coefficeints and performance to a simple linear regression model. Moreover, with the dataset relatively small, the stochastic nature of SGD will not benefit the model, as this will behave like a full-batch gradient descent (equivalent to simple linear regression model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variability in the dataset is evidently observed. Because of this, linear regression struggles because it attempts to fit all the data including the noise leading to overfitting. Lasso and SGD adds bias into making the model simpler. This instead may cause the model to underfit. With this, linear regression, even with Lasso regularization and SGD, fails to perform well on small, high-variability datasets due to its inability handle noisy and nonlinear relationships. Regularization can mitigate overfitting to some degree but cannot compensate for the limited information in small datasets or reduce inherent variability in the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IX-B. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getModelPerformance(tuned_rf_model, 'Initial Random Forest', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train_pred = tuned_rf_model.predict(X_train)\n",
    "rf_test_pred = tuned_rf_model.predict(X_test)\n",
    "\n",
    "print(\"Predicted - Train:\\n\")\n",
    "print(showPredToActualTable(y_train, rf_train_pred, 20, False))\n",
    "print(\"\\n\")\n",
    "print(\"Predicted - Test:\\n\")\n",
    "print(showPredToActualTable(y_test, rf_test_pred, 20, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotProductivityError(y_train, rf_train_pred, 'Initial Random Forest (Train)', False)\n",
    "plotProductivityError(y_test, rf_test_pred, 'Initial Random Forest(Test)', False)\n",
    "\n",
    "plotProductivityDistribution(y_train, rf_train_pred, 'Initial Neural Network (Train)', False)\n",
    "plotProductivityDistribution(y_test, rf_test_pred, 'Initial Neural Network (Test)', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IX-C. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the performance of the initial and tuned neural network models\n",
    "getModelPerformance(initial_nn_model, 'Initial Neural Network', True)\n",
    "print('\\n--------------------------------------------------\\n')\n",
    "getModelPerformance(tuned_nn_model, 'Tuned Neural Network', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shown above are the final performance metrics of both the Initial and Tuned Neural Networks after training.\n",
    "\n",
    "Comparing the performance of the Initial and Tuned Neural Networks, it can be noted that the Initial Model's predictions fits the train set a lot more than the Tuned Model. \n",
    "This may be because the RandomizedSearch using Cross-Validation, training the model with only a subset of the train set and using the rest for evaluation.\n",
    "This lets the Tuned Model not overfit to the train set, thus why it has a higher error.\n",
    "\n",
    "Meanwhile, for the test set, the Tuned Model performs only marginally better than the Initial Model.\n",
    "This may be because the default parameters of the Initial Model are already pretty good in generating a workable model by themselves.\n",
    "One thing to point out though is that the R^2 score of the Tuned Model is twice as good as the Initial Model's.\n",
    "This suggests that the Tuned Model fits the test set's data trend more as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_y_train_pred = tuned_nn_model.predict(X_train_scaled)\n",
    "nn_y_test_pred = tuned_nn_model.predict(X_test_scaled)\n",
    "\n",
    "# Scale the predicted values back to 0 and 1\n",
    "nn_y_train_pred = np.clip(nn_y_train_pred, 0, 1)\n",
    "nn_y_test_pred = np.clip(nn_y_test_pred, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotProductivityError(y_train, nn_y_train_pred, 'Tuned Neural Network (Train)', False)\n",
    "plotProductivityError(y_test, nn_y_test_pred, 'Tuned Neural Network (Test)', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shown above are the comparisons of the predicted and actual productivity values of the Tuned Neural Network to the train and test set.\n",
    "\n",
    "In general, the Tuned Model's predictions show many of the same qualities of those of the Initial Model.\n",
    "Mainly that is still conservative with its predicted productivity values, in contrast to the large variance of the actual productivity values in the dataset.\n",
    "However, a somewhat key difference is that the model performed worse in the test set compared to the Initial Model, as mentioned in the error metrics.\n",
    "In exchange, the Tuned Model now better follows the general trend of the data, with its predictions being able to follow the actual values in most cases, in exception to the large variance data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotProductivityDistribution(y_train, nn_y_train_pred, 'Tuned Neural Network (Train)', False)\n",
    "plotProductivityDistribution(y_test, nn_y_test_pred, 'Tuned Neural Network (Test)', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shown above are the histograms of the predicted and actual productivity values of the Tuned Neural Network to the train and test set.\n",
    "\n",
    "Similar to the analysis above, the Tuned Model's predictions show many of the same qualities of those of the Initial Model.\n",
    "The key difference of the Tuned Model to the Initial Model's historgram is that the Tuned Model's predictions are more spread out in comparison.\n",
    "This highlights how it prefers to make conservative predictions based on certain trends in the data and distribute its values.\n",
    "However, this approach may not be the most effective as the distribution of the actual productivity of the dataset tends to skew to certain values, as shown in the histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X. Insights and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XI. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "1. 1.17. Neural network models (supervised). (n.d.). Scikit-learn. https://scikit-learn.org/stable/modules/neural_networks_supervised.html#neural-networks-supervised\n",
    "   - Multi-layer Perceptron (MLP) Models of scikit-learn\n",
    "\n",
    "2. MLPRegressor. (n.d.). Scikit-learn. https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor\n",
    "   - Multi-layer Perceptron (MLP) Regressor of scikit-learn\n",
    "\n",
    "3. Karpathy, A. (2019, April 25). A recipe for training neural networks. Andrej Karpathy blog. https://karpathy.github.io/2019/04/25/recipe/\n",
    "   - General tips to train Neural Networks\n",
    "\n",
    "4. Dutta, S. (2024, November 24). Number of neurons per hidden layer in neural networks: a guide. Medium. https://medium.com/@sanjay_dutta/number-of-neurons-per-hidden-layer-in-neural-networks-a-guide-106fea04fbfe\n",
    "   - Tips for number of neurons and hidden layers for Neural Networks\n",
    "\n",
    "5. An, S. (2022, June 2). How to tune hyperparameters for better neural network performance. Medium. https://medium.com/codex/how-to-tune-hyperparameters-for-better-neural-network-performance-b8f542855d2e\n",
    "   - Tips for hyperparameter tuning of Neural Networks\n",
    "\n",
    "6. GeeksforGeeks. (2023, December 7). Hyperparameter tuning. GeeksforGeeks. https://www.geeksforgeeks.org/hyperparameter-tuning/\n",
    "   - Tips for hyperparameter tuning in general\n",
    "\n",
    "7. Functions of Finishing Department in Garment Industry. (2015, October 24). Online Clothing Study. https://www.onlineclothingstudy.com/2015/10/functions-of-finishing-department-in.html\n",
    "   - Some info on what the finishing department does\n",
    "\n",
    "8. Breiman, L. (2001). Random Forests. Machine Learning, 45(1), 5â€“32. https://doi.org/10.1023/A:1010933404324\n",
    "   - Original Random Forest paper, also explains the convergence and prevention of overfitting\n",
    "\n",
    "9. Probst, P., Wright, M. N., & Boulesteix, A.-L. (2019). Hyperparameters and tuning strategies for random forest. WIREs Data Mining and Knowledge Discovery, 9(3), e1301. https://doi.org/10.1002/widm.1301\n",
    "   - RF Tuning strategies 1\n",
    "\n",
    "10. Scornet, E. (2017). Tuning parameters in random forests. ESAIM: Proceedings and Surveys, 60, 144â€“162. https://doi.org/10.1051/proc/201760144\n",
    "   - RF Tuning strategies 2\n",
    "\n",
    "11. What Is Random Forest? | IBM. (2021, October 20). https://www.ibm.com/topics/random-forest\n",
    "   - Surface Level Understanding of RF\n",
    "\n",
    "12. Ross, Sheldon M. â€œLinear Regression.â€ Introductory Statistics, 2010, pp. 537â€“604, www.sciencedirect.com/science/article/pii/B9780123743886000120, https://doi.org/10.1016/b978-0-12-374388-6.00012-0.\n",
    "- Surface Level Understanding of Linear Regression\n",
    "\n",
    "13. Iqbal, M. (2020). Application of Regression Techniques with their Advantages and Disadvantages. https://www.researchgate.net/profile/Muhammad-Ahmad-Iqbal/publication/354921553_Application_of_Regression_Techniques_with_their_Advantages_and_Disadvantages/links/61543c3c14d6fd7c0fb91053/Application-of-Regression-Techniques-with-their-Advantages-and-Disadvantages.pdf\n",
    "- Advantages and Disadvantages of Linear Regression\n",
    "\n",
    "14. Tibshirani, R. (1996). Regression Shrinkage and Selection Via the Lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1), 267â€“288. https://doi.org/10.1111/j.2517-6161.1996.tb02080.x\n",
    "- Lasso Regularization for Linear Regression\n",
    "\n",
    "15. Jupudi, Lakshmi(2016). Stochastic Gradient Descent using Linear Regression with Python. IJAERA. 2. 519 -525. \n",
    "- Stochastic Gradient Descent for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
